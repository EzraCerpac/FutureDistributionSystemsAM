{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EE4375-2022: Fifth Lab Session: One-Dimensional Galerkin Finite Element Method using Distributed Memory Parallel Computing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solves the Poisson equation $- \\frac{d^2 \\, u(x)}{dx^2} = f(x)$ on the unit bar domain $x \\in \\Omega=(0,1)$ supplied with various boundary conditions and various source terms. The Galerkin finite element method is employed. Here we target a parallel computing (using distributed memory) imnplementation. \n",
    "\n",
    "This problem can be solved using [GridapDistributed](https://gridap.github.io/GridapDistributed.jl/dev/) as students in the bachelor minor Computational Science and Engineering have convincingly shown. It remains valuable to dig for the details. \n",
    "\n",
    "General info on [parallel computing in Julia](https://juliaparallel.org/resources/) and [MPI.jl](https://github.com/JuliaParallel/MPI.jl). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra\n",
    "using Plots\n",
    "using LaTeXStrings\n",
    "using SparseArrays\n",
    "using BenchmarkTools\n",
    "using SparseArrays\n",
    "using Distributed\n",
    "using SharedArrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Preprocessing \n",
    "- <b> shared memory approach </b>: monolytic in memory approach: assuming global mesh, matrix and rhs vector to be available on all processors: assembly process similar to all processors; \n",
    "- <b> distributed memory approach </b>: distribute memory approach: distribute memory approach: distributed assembly and solve; include figure here; "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Shared Memory Assembly of Matrix and Right-Hand Side Vector \n",
    "- <b> decompose mesh elements over the processors </b>: observe mesh to be decomposed according to elements and <b>not</b> according to nodes; \n",
    "- <b> use shared memory for-loop over elements </b>: the for-loop over elements can run in parallel in shared memory using either the macro @distributed or pmap function from the  [distributed computing section](https://docs.julialang.org/en/v1/stdlib/Distributed/) of the standard library; \n",
    "- <b> used SharedVector and SharedMatrix to store answers</b>: using SharedVector and SharedMatrix using [shared arrays](https://docs.julialang.org/en/v1/stdlib/SharedArrays/); each processor fills its part of the matrix; \n",
    "- not sure whether [Distributed Arrays](https://juliaparallel.org/DistributedArrays.jl/stable/) offers any advantages; \n",
    "\n",
    "<div>\n",
    "<img src=\"./figures/dag-fem-1d.jpg\" width=400 /> \n",
    "<center> Figure 1: Directed acyclic graph representation of decomposed mesh of 4 elements on the interval. A finite element discretization is assumed. </center>   \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "On worker 2:\nUndefVarError: spzeros not defined\nStacktrace:\n [1] top-level scope\n\u001b[90m   @ \u001b[39m\u001b[90m\u001b[4mnone:1\u001b[24m\u001b[39m\n [2] \u001b[0m\u001b[1meval\u001b[22m\n\u001b[90m   @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90m\u001b[4mboot.jl:368\u001b[24m\u001b[39m\n [3] \u001b[0m\u001b[1m#invokelatest#2\u001b[22m\n\u001b[90m   @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90m\u001b[4messentials.jl:729\u001b[24m\u001b[39m\n [4] \u001b[0m\u001b[1minvokelatest\u001b[22m\n\u001b[90m   @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90m\u001b[4messentials.jl:726\u001b[24m\u001b[39m\n [5] \u001b[0m\u001b[1m#114\u001b[22m\n\u001b[90m   @ \u001b[39m\u001b[90m/Applications/Julia-1.8.app/Contents/Resources/julia/share/julia/stdlib/v1.8/Distributed/src/\u001b[39m\u001b[90m\u001b[4mprocess_messages.jl:301\u001b[24m\u001b[39m\n [6] \u001b[0m\u001b[1mrun_work_thunk\u001b[22m\n\u001b[90m   @ \u001b[39m\u001b[90m/Applications/Julia-1.8.app/Contents/Resources/julia/share/julia/stdlib/v1.8/Distributed/src/\u001b[39m\u001b[90m\u001b[4mprocess_messages.jl:70\u001b[24m\u001b[39m\n [7] \u001b[0m\u001b[1mrun_work_thunk\u001b[22m\n\u001b[90m   @ \u001b[39m\u001b[90m/Applications/Julia-1.8.app/Contents/Resources/julia/share/julia/stdlib/v1.8/Distributed/src/\u001b[39m\u001b[90m\u001b[4mprocess_messages.jl:79\u001b[24m\u001b[39m\n [8] \u001b[0m\u001b[1m#100\u001b[22m\n\u001b[90m   @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90m\u001b[4mtask.jl:484\u001b[24m\u001b[39m\n\n...and 3 more exceptions.\n",
     "output_type": "error",
     "traceback": [
      "On worker 2:\nUndefVarError: spzeros not defined\nStacktrace:\n [1] top-level scope\n\u001b[90m   @ \u001b[39m\u001b[90m\u001b[4mnone:1\u001b[24m\u001b[39m\n [2] \u001b[0m\u001b[1meval\u001b[22m\n\u001b[90m   @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90m\u001b[4mboot.jl:368\u001b[24m\u001b[39m\n [3] \u001b[0m\u001b[1m#invokelatest#2\u001b[22m\n\u001b[90m   @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90m\u001b[4messentials.jl:729\u001b[24m\u001b[39m\n [4] \u001b[0m\u001b[1minvokelatest\u001b[22m\n\u001b[90m   @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90m\u001b[4messentials.jl:726\u001b[24m\u001b[39m\n [5] \u001b[0m\u001b[1m#114\u001b[22m\n\u001b[90m   @ \u001b[39m\u001b[90m/Applications/Julia-1.8.app/Contents/Resources/julia/share/julia/stdlib/v1.8/Distributed/src/\u001b[39m\u001b[90m\u001b[4mprocess_messages.jl:301\u001b[24m\u001b[39m\n [6] \u001b[0m\u001b[1mrun_work_thunk\u001b[22m\n\u001b[90m   @ \u001b[39m\u001b[90m/Applications/Julia-1.8.app/Contents/Resources/julia/share/julia/stdlib/v1.8/Distributed/src/\u001b[39m\u001b[90m\u001b[4mprocess_messages.jl:70\u001b[24m\u001b[39m\n [7] \u001b[0m\u001b[1mrun_work_thunk\u001b[22m\n\u001b[90m   @ \u001b[39m\u001b[90m/Applications/Julia-1.8.app/Contents/Resources/julia/share/julia/stdlib/v1.8/Distributed/src/\u001b[39m\u001b[90m\u001b[4mprocess_messages.jl:79\u001b[24m\u001b[39m\n [8] \u001b[0m\u001b[1m#100\u001b[22m\n\u001b[90m   @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90m\u001b[4mtask.jl:484\u001b[24m\u001b[39m\n\n...and 3 more exceptions.\n",
      "",
      "Stacktrace:",
      " [1] sync_end(c::Channel{Any})",
      "   @ Base ./task.jl:436",
      " [2] macro expansion",
      "   @ ./task.jl:455 [inlined]",
      " [3] remotecall_eval(m::Module, procs::Vector{Int64}, ex::Expr)",
      "   @ Distributed /Applications/Julia-1.8.app/Contents/Resources/julia/share/julia/stdlib/v1.8/Distributed/src/macros.jl:219",
      " [4] top-level scope",
      "   @ /Applications/Julia-1.8.app/Contents/Resources/julia/share/julia/stdlib/v1.8/Distributed/src/macros.jl:203"
     ]
    }
   ],
   "source": [
    "# start REPL using julia -p 5 \n",
    "addprocs(4)\n",
    "no_procs = nprocs()\n",
    "\n",
    "#..construct the mesh: see before \n",
    "@everywhere N = 100; \n",
    "@everywhere h = 1/N; \n",
    "@everywhere x = Vector(0:h:1); \n",
    "\n",
    "#..Mesh with points and edges \n",
    "#..point holds the coordinates of the left and right node of the element\n",
    "#..edges holds the global indices of the left and right node of the element\n",
    "@everywhere points = collect( [x[i], x[i+1]] for i in 1:length(x)-1) \n",
    "@everywhere edges = collect( [i, i+1] for i in 1:length(x)-1); \n",
    "\n",
    "#..Set the source function \n",
    "@everywhere fsource(x) = x*(x-1); \n",
    "\n",
    "#..Initialize global matrix and right-hand side value \n",
    "@everywhere A = spzeros(length(x), length(x)); \n",
    "@everywhere f = zeros(length(x), 1); \n",
    "\n",
    "#..Perform loop over elements and assemble global matrix and vector \n",
    "@distributed for i=1:length(edges) \n",
    "\n",
    "  xl, xr = points[i,:][1]\n",
    "  floc = (xr-xl) * [fsource(xl) fsource(xr)];\n",
    "  Aloc = (1/(xr-xl))*[1 -1; -1 1]; \n",
    "\n",
    "  for j=1:2 \n",
    "    f[edges[i][j]] += floc[j];\n",
    "    for k =1:2 \n",
    "      A[edges[i][j], edges[i][k]] += Aloc[j,k]; \n",
    "    end \n",
    "  end \n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nprocs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Shared Memory Assembly of Matrix and Right-Hand Side Vector \n",
    "- using [PartionedMatrices](https://github.com/fverdugo/PartitionedArrays.jl); need to study the example described at [example](https://www.francescverdugo.com/PartitionedArrays.jl/dev/usage/).   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Shared Memory Linear System Solve \n",
    "- <b> first approach </b>: use backslash (nothing to be done, need to check solvers for sharedArrays); \n",
    "- using sequential preconditioned [conjugate gradient method](https://en.wikipedia.org/wiki/Conjugate_gradient_method) from [IterativeSolvers.jl]([https://github.com/JuliaLinearAlgebra/IterativeSolvers.jl)\n",
    "- <b> second approach </b>: use preconditioned conjugate gradient method; using parallel BLAS1 and BLAS2 functions; using [sparse-matrix multiplication](https://github.com/JuliaInv/ParSpMatVec.jl);\n",
    "- use PCG with proper [overlap of computation and communication](https://netlib.org/linalg/html_templates/node107.html#SECTION00941100000000000000);  \n",
    "\n",
    "### Parallel BLAS-1 Operations \n",
    "Using [ParallelUtilities](https://docs.juliahub.com/ParallelUtilities/SO4iL/0.7.0/). \n",
    "\n",
    "#### Vector Norm \n",
    "Given vector ${\\mathbf v}$, compute its squared norm $\\| \\mathbf v \\|^2$, using pmapsum(x->x^2,v). Alternatively, this is the function dot with the same input argument repeated.  \n",
    "\n",
    "#### Vectors Inner Product \n",
    "Given vectors ${\\mathbf v}$ and ${\\mathbf w}$, compute their inner product ${\\mathbf v} \\cdot {\\mathbf w}$, using pmapsum((x,y)->x*y,v,w). See [this post](https://discourse.julialang.org/t/function-pmap-multi-argument/74153) for an example of pmap using two arguments. Alternatively, this is the function dot. \n",
    " \n",
    "#### Vector Update \n",
    "Given vectors ${\\mathbf v}$ and ${\\mathbf w}$, and given a number $\\alpha$ (typically the result of an inner product evaluation), compute the vector update ${\\mathbf w} = {\\mathbf w} + \\alpha {\\mathbf v}$, using pmapsum((x,y,a)->y+a*x,v,w,alpha). Alternatively, we can update the components of the vector ${\\mathbf w}$ elementwise. \n",
    "\n",
    "### Parallel BLAS-2 Operations \n",
    "\n",
    "#### Sparse Matrix-Vector Multiplication \n",
    "Use [ParSpMatVec](https://github.com/JuliaInv/ParSpMatVec.jl/blob/master/test/test_A_mul_B.jl) for sparse matrix vector multiplication. Alternatively, this is the function mul!. \n",
    "Background information is provided in e.g. [Parallel Linear Algebra by Deprez](https://fdesprez.github.io/teaching/par-comput/lectures/slides/L8-AlLinPar-2p.pdf); \n",
    "\n",
    "### Steepest Descent Method \n",
    "Combine ingredients above to implement the steepest descent method as described in Section [Solution to a Linear System](https://en.wikipedia.org/wiki/Gradient_descent#Solution_of_a_linear_system). \n",
    "\n",
    "### Conjugate Gradient Method \n",
    "Extend steepest descent to the conjugate gradient method as described in Section [Resulting Algorithm](https://en.wikipedia.org/wiki/Conjugate_gradient_method#The_resulting_algorithm). An explanation of the CG algorithm using iterates in explained on [iterative-methods-done-right](https://lostella.github.io/2018/07/25/iterative-methods-done-right.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Distributed Memory Linear System Solve \n",
    "Using the PETSc library (as done by the bachelor students). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Postprocessing \n",
    "Visualize the computed solution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.1",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
